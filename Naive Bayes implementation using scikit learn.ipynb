{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is naive Bayes algorithm?***\n",
    "\n",
    " It is a generative, supervised learning method.\n",
    "\n",
    "A classification technique, based on Bayes' theorem i.e $P(y|x)= \\frac{P(x|y)P(y)}{P(x)}.$\n",
    "\n",
    "\n",
    "\n",
    "It assumes that features are independent i.e the presence of a particular feature in a class is unrelated to the presence of any other feature in that class. Thus, the name 'Naive'.\n",
    "\n",
    "Using the Bayes theorem and the assumption of indendepence of features we shall have the following\n",
    "\n",
    "$P(y|x_1,...,x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i|y)}{P(x_1,...,x_n)}$\n",
    "\n",
    "\n",
    "where;\n",
    "\n",
    "\n",
    "\n",
    "$P(y|x_1,...,x_n)$ --- is the posterior probability of class (c,target) given feature(x,attributes).\n",
    "\n",
    "$P(x_1,...,x_n/y)$ --- likelihood i.e probability of feature give class\n",
    "\n",
    "$P(y)$ --- prior probability of a class\n",
    "\n",
    "$P(x_1,...,x_n)$ --- prior probability of feature\n",
    "\n",
    "\n",
    "Since $P(x_1,...,x_n)$ is a constant given the input, we can use the following classification rule:\n",
    "\n",
    "$P(y|x_1,...,x_n) \\propto P(y)\\prod_{i=1}^nP(x_i|y)$\n",
    "\n",
    "$\\implies \\hat{y} = \\arg\\max_{y }P(y)\\prod_{i=1}^nP(x_i|y)$\n",
    "\n",
    "To estimate $P(y)$ and $P(x_i|y)$ we use the Maximum A Posteriori (MAP). \n",
    "\n",
    "Naive bayes is commonly used in text classiffication and with cases where there are multiple classes.\n",
    "\n",
    "\n",
    "***Pros and cons of Naive Bayes***\n",
    "\n",
    "***Pros***\n",
    "\n",
    "-Faster and performs well in multiclass prediction\n",
    "\n",
    "-Performs better compared to other models such as logistic regression when the independence asssumption holds.\n",
    "\n",
    "-Requires less training training data.\n",
    "\n",
    "-Performs well in case of categorical variables compared to numerical variables(s).\n",
    "For numerical variables, normal distribution is assumed.\n",
    "\n",
    "***Cons***\n",
    "\n",
    "-The model assigns a probability of 0 to a category of variable which is in a test data and not in a training data. (This is known as zero frequency). Smoothing techique is used to solve this,for example using the laplace estimation technique.\n",
    "\n",
    "-In life it is difficult to find that feutures are independent as assumed by Naive bayes.\n",
    "\n",
    "***Application of Naive Bayes Algorithims***\n",
    "\n",
    "**Real time prediction**\n",
    "\n",
    "Used for prediction in real time due to the fact that it is faster.\n",
    "\n",
    "**Multiclass prediction**\n",
    "\n",
    "Can be used for prediction of probability of multiclasses of target variable.\n",
    "\n",
    "**Text classification/ Spam Filtering/ Sentiment Analysis**\n",
    "\n",
    "-It is widely used for text classification due to better result in multi class problems and indepence rule.\n",
    "\n",
    "-It is also widely used for spam filtering (identifu spam email) and sentiment analysis to identify positive and negative customer sentiments.\n",
    "\n",
    "**Recommendation System**\n",
    "\n",
    "Together with collaborative filtering, it builds a recommendation system to filter unseen information and predict whether a user would like a given resource or not.\n",
    "\n",
    "\n",
    "***Three kinds of Naive Bayes model***\n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$.\n",
    "\n",
    "**Gaussian**\n",
    "\n",
    "It assumes that features follow a normal distribution (that is are numerical)\n",
    "\n",
    "**Multinomial**\n",
    "\n",
    "It is used for discrete counts features.\n",
    "\n",
    "**Bernoulli**\n",
    "\n",
    "It is used when the feature vectors are binary(that is, zeros and ones).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall implement the three kinds of Naive Bayes using scikitlearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gaussian Naive Bayes implementation***\n",
    "\n",
    "For this case, the probability of feautures is assumed to take a normal (gaussian)distribution. Features are numerical (continous).\n",
    "\n",
    "$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}\\exp\n",
    "\\left(-\\frac{(x_i-\\mu_y)}{2\\sigma^2_y}\\right)$\n",
    "\n",
    "We shall use the iris data which has continous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import iris dataset\n",
    "dataset = pd.read_csv(\"/home/aims/Documents/Kaggle Data/IRIS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data information\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information above we can see that our data does not have missing values, the dataset has 150 training examples/samples and 5 columns(features and target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the first few rows of our data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for normality of features\n",
    "from scipy.stats import normaltest \n",
    "print('normality result for petal_width :', normaltest(dataset['petal_width']))\n",
    "print('normality result for petal_length :', normaltest(dataset['petal_length']))\n",
    "print('normality result for sepal_length :', normaltest(dataset['sepal_length']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib histogram\n",
    "#plt.hist(dataset['petal_width'], color = 'blue', edgecolor = 'black',\n",
    "         #bins = int(180/5))\n",
    "\n",
    "# seaborn histogram\n",
    "sns.distplot(dataset['petal_width'], hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Histogram of petal_width')\n",
    "plt.xlabel('petal width')\n",
    "plt.ylabel('petal width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now split dataset into features and target\n",
    "features = dataset.drop(['species'],1).values\n",
    "target = dataset['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now encode the target labes with values 0 and n-classes-1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "target= labelencoder.fit_transform(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now split our data into train and test set\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the model\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with train dataset\n",
    "\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target on the training dataset\n",
    "predict_train = model.predict(x_train)\n",
    "print('prediction classes of target based on train data: ', predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score on train dataset\n",
    "train_accuracy = accuracy_score(y_train,predict_train)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target on the testing dataset\n",
    "predict_test = model.predict(x_test)\n",
    "print('prediction classes of target based on test data: ', predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score on test dataset\n",
    "test_accuracy = accuracy_score(y_test,predict_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model performs better both in train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips to improve the power of Naive Bayes Model**\n",
    "\n",
    "-If continous feature do not have distribution, we need to consider use of transformation or different methods to convert it in normal distribution.\n",
    "\n",
    "-If dataset has zero frequence issue , we apply smoothing technidue, laplace correction to predict class of test data set.\n",
    "\n",
    "-Remove corelated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
